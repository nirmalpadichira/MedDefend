#### The RPCA parameters lamp, beta,mu, and ro are to be tuned to obtain optimum results, the error is fixed at 10^-6, which can also be changed as desired.#######

def RPCA_N(H):
    J=np.zeros_like(H)
    #Z=J
    W=np.zeros_like(H)
    S=np.zeros_like(H)
    G1=np.zeros_like(H)
    G2=np.zeros_like(H)

    lamp=10
    #lamp1=0.000000001
    beta=0.125
    mu=22.5

    max_mu=100
    ro=1.1
    while not converged(H, J, S, W):
          J=svd_shrink(.5*(H-S-W+0+((G1-G2)/mu)),1/(2*(mu)))
          S=shrink(H-J-W+G1/mu , lamp/mu)
          W = (mu*(H-J-S+(G1/mu)))/(2*beta + mu)

      ## Update equations
          G1=G1+(mu*(H-J-S-W))
          G2=G2+(mu*(J-0))

          mu=min(mu*ro,max_mu)
          #count += 1
    return J,S,W


def shrink(X, tau):
    V = np.copy(X).reshape(X.size)
    for i in range(V.size):
        V[i] = np.copysign(max(abs(V[i]) - tau, 0),V[i])
        if V[i] == -0:
            V[i] = 0
    return V.reshape(X.shape)

def svd_shrink(X, tau):
    U, s, V = np.linalg.svd(X, full_matrices=False)
    return np.matmul(U, np.matmul(np.diag(shrink(s, tau)),V))

import numpy as np

def frobeniusNorm(X):
    return np.linalg.norm(X)

def converged(H, J, S,W):
    error = frobeniusNorm(H-J-S-W) / frobeniusNorm(H)
    print("error =", error)
    return error <= 10e-6
